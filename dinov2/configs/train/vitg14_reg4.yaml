dino:
  head_n_prototypes: 131072
  head_bottleneck_dim: 384
  kde_loss_weight: .05
  koleo_loss_weight: 0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5 
  mask_ratio_min_max:
    - 0.1 
    - 0.45 
  separate_head: true
  head_n_prototypes: 131072 
lejepa:
  lambd: 0.05
  num_slices: 1024
  t_max: 3.0
  n_points: 17
  proj_dim: 256
  proj_hidden_dim: 2048
  proj_nlayers: 3
  proj_norm: layernorm
model:
  family: dinov3 # dinov2 or dinov3
train:
  objective: lejepa # dinov2 or lejepa
  pretrained_source: dinov3 # dinov2, dinov3, or pixio
  pretrained_ckpt: /home/paul/dinov3/checkpoints/dinov3_vit7b16_saved_teacher.pth # dinov3: /home/paul/dinov3/checkpoints/dinov3_vit7b16_saved_teacher.pth | pixio: /home/paul/path-pixio/pretrained_ckpts/pixio_vith16.pth | pixio 5b: /home/paul/path-pixio/pretrained_ckpts/pixio_vit5b16.pth
  dataset_root: /data/TCGA
  sample_list_path: /data/TCGA/sample_dataset_30.txt # gives paths to svs files for data loading if streaming_from_hf=False
  streaming_from_hf: true
  streaming_dataset_path: /data/TCGA_parquet_sample30_shuffled #medarc/TCGA-12K-parquet
  batch_size_per_gpu: 48
  centering: sinkhorn_knopp
  use_pretrained: True
  OFFICIAL_EPOCH_LENGTH: 1250
  num_workers: 4
  skip_checkpointer: true
# student: #dinov2
#   arch: vit_giant2 
#   patch_size: 14 
#   drop_path_rate: 0.4 
#   ffn_layer: swiglufused 
#   block_chunks: 4
#   num_register_tokens: 4 
#   layerscale: 1.0e-05 
# student: #pixio
#   arch: vit_huge 
#   patch_size: 16
#   drop_path_rate: 0.0
#   ffn_layer: mlp
#   block_chunks: 4
#   num_register_tokens: 7
#   layerscale: 0.0
# student: #pixio vit5b16
#   arch: vit_5b
#   patch_size: 16
#   drop_path_rate: 0.0
#   ffn_layer: mlp
#   block_chunks: 4
#   num_register_tokens: 7
#   layerscale: 0.0
student: #dinov3 vit7b16
  arch: vit_7b
  patch_size: 16
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  ffn_layer: swiglu64
  ffn_ratio: 3.0
  norm_layer: layernormbf16
  n_storage_tokens: 4
  qkv_bias: false
  proj_bias: true
  ffn_bias: true
  mask_k_bias: true
  untie_cls_and_patch_norms: false
  untie_global_and_local_cls_norm: true
  pos_embed_rope_base: 100.0
  pos_embed_rope_min_period: null
  pos_embed_rope_max_period: null
  pos_embed_rope_normalize_coords: separate
  pos_embed_rope_shift_coords: null
  pos_embed_rope_jitter_coords: null
  pos_embed_rope_rescale_coords: 2.0
  pos_embed_rope_dtype: fp32
  fp8_enabled: false
  fp8_filter: blocks
teacher:
  momentum_teacher: 0.994
optim:
  epochs: 8000 
  early_stop: 200 # we want to early stop at 200 epochs to replicate SOTA ckpt
  weight_decay_end: 0.2
  base_lr: 1.0e-03  # learning rate for a batch size of 1024 (swapped from 3.5e-4 to 2.0e-4)
  warmup_epochs: 10
  layerwise_decay: 1.0
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 6
  local_crops_scale:
  - 0.1 # 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 112 # dinov2: 98 | pixio/dinov3: use a multiple of 16 (112)
evaluation:
  eval_period_iterations: 625 
  bach_root: /data/eva-data/bach
  breakhis_root: /data/eva-data/breakhis
  pcam_root: /data/eva-data/patch_camelyon
lora:
  enabled: true
  rank: 64
  alpha: 64
  dropout: 0.0
  target: attn # mlp+attn or attn or mlp
  unfrozen_blocks: [-2, -1]
  # lora_blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29] # pixio vit huge
  # lora_blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45] # pixio 5b
  # lora_blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47] # pixio vit5b
  # lora_blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37] # dinov2 giant
  lora_blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37] # dinov3 vit7b
  
