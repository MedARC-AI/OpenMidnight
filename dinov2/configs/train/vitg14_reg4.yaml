dino:
  head_n_prototypes: 131072
  head_bottleneck_dim: 384
  kde_loss_weight: .05
  koleo_loss_weight: 0
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5 
  mask_ratio_min_max:
    - 0.1 
    - 0.45 
  separate_head: true
  head_n_prototypes: 131072 
lejepa:
  lambd: 0.05
  num_slices: 1024
  t_max: 3.0
  n_points: 17
  proj_dim: 256
  proj_hidden_dim: 2048
  proj_nlayers: 3
  proj_norm: layernorm
train:
  objective: lejepa # dinov2 or lejepa
  sample_list_path: /data/TCGA/sample_dataset_30.txt # gives paths to svs files for data loading if streaming_from_hf=False
  streaming_from_hf: true
  streaming_dataset_path: /data/TCGA_parquet_sample30_shuffled #medarc/TCGA-12K-parquet
  batch_size_per_gpu: 48
  centering: sinkhorn_knopp
  use_pretrained: True
  OFFICIAL_EPOCH_LENGTH: 1250
  num_workers: 4
  skip_checkpointer: true
student:
  arch: vit_giant2
  patch_size: 14
  drop_path_rate: 0.4
  ffn_layer: swiglufused
  block_chunks: 4
  num_register_tokens: 4
teacher:
  momentum_teacher: 0.994
optim:
  epochs: 8000 
  early_stop: 200 # we want to early stop at 200 epochs to replicate SOTA ckpt
  weight_decay_end: 0.2
  base_lr: 1.0e-03  # learning rate for a batch size of 1024 (swapped from 3.5e-4 to 2.0e-4)
  warmup_epochs: 10
  layerwise_decay: 1.0
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 6
  local_crops_scale:
  - 0.1 # 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98 #112
evaluation:
  eval_period_iterations: 625 
  bach_root: /data/eva-data/bach
lora:
  enabled: true
  rank: 64
  alpha: 64
  dropout: 0.0
  target: attn # mlp+attn or attn or mlp
  unfrozen_blocks: [-2, -1]
  lora_blocks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]
